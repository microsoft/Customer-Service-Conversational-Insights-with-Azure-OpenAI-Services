{"cells":[{"cell_type":"code","execution_count":null,"id":"6042c33d-d831-4da4-890d-da2fb8e3d3c8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%pip install azure-cognitiveservices-speech==1.37.0"]},{"cell_type":"code","execution_count":null,"id":"46b5e38d-0232-4336-9d64-0c0a3687efed","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["# key_vault_name value is set at the time of deployment\n","\n","key_vault_name = 'kv_to-be-replaced'"]},{"cell_type":"code","execution_count":null,"id":"34455249-5154-4cf8-b102-abcef9ca9d89","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n","\n","def get_secrets_from_kv(kv_name, secret_name):\n","\n","    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n","    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n","    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))\n","\n","openai_api_type = \"azure\"\n","openai_api_version  = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-VERSION\")\n","openai_api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n","openai_api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-KEY\")"]},{"cell_type":"code","execution_count":null,"id":"e4d20f3e-8425-4ed7-add7-2056418afdb9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Set AI services variables\n","ai_services_endpoint = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-ENDPOINT\") \n","ai_services_key = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-KEY\") \n","ai_services_region = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-REGION\")\n","# wav_file_path = '/lakehouse/default/Files/data/audio_input/Travel_20240417132839.wav'\n","language1 = 'en-US'"]},{"cell_type":"code","execution_count":null,"id":"6ab6c88e-45f7-4e5b-829c-8d3bdd95de25","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # This cell creates new folders within the specified base path in the lakehouse. \n","# The purpose is to create corresponding folders so files can be moved as they are processed.\n","import os \n","\n","# Define the base path\n","base_path = '/lakehouse/default/Files/data'\n","\n","# List of folders to be created\n","folders = ['audio_failed', 'audio_processed']\n","\n","# Create each folder\n","for folder in folders:\n","    folder_path = os.path.join(base_path, folder)\n","    try:\n","        os.makedirs(folder_path, exist_ok=True)\n","        print(f'Folder created at: {folder_path}')\n","    except Exception as e:\n","        print(f'Failed to create the folder {folder_path}. Error: {e}')"]},{"cell_type":"code","execution_count":null,"id":"be8cacd3-e46b-49dd-a13a-1f85477dda39","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Drop the metadata table if it already exists\n","spark.sql('drop table if exists ckm_conv_metadata')"]},{"cell_type":"code","execution_count":null,"id":"55e3c4a2-e888-4bd8-add1-fb08f42af75c","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","\n","spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n","\n","# Read all the CSV files in the directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/audio_input/*.csv\")\n","\n","# Convert StartTime and EndTime to timestamp format\n","df = df.withColumn(\"StartTime\", F.to_timestamp(\"StartTime\", \"MM/dd/yyyy h:mm:ss a\"))\n","df = df.withColumn(\"EndTime\", F.to_timestamp(\"EndTime\", \"MM/dd/yyyy h:mm:ss a\"))\n","\n","# Calculate the duration in milliseconds and add it as a new column\n","df = df.withColumn(\"Duration\", (F.col(\"EndTime\").cast(\"long\") - F.col(\"StartTime\").cast(\"long\")) / 60)\n","\n","\n","# Write the DataFrame to a Delta table\n","df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('ckm_conv_metadata')\n","\n","# # Display the first 2 rows\n","# display(df.head(2))\n"]},{"cell_type":"code","execution_count":null,"id":"6e0f17c4-fc8d-4b0d-9ea1-aa72bd640963","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# This code block is designed to transcribe speech from an audio file using Azure's Cognitive Services Speech SDK.\n","# https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization?tabs=windows&pivots=programming-language-python\n","\n","# It supports diarization, which distinguishes between different speakers in the audio.\n","# The transcribed results are returned as a list of all recognized utterances with associated metadata.\n","\n","import os\n","import time\n","import azure.cognitiveservices.speech as speechsdk\n","import json\n","\n","# Function to transcribe speech from an audio file\n","def transcribe_from_file(ai_services_key, ai_services_region, wav_file_path, conversation_id):\n","    # List to store the results of the transcription\n","    all_results = list()\n","\n","    # Configure the speech service\n","    speech_config = speechsdk.SpeechConfig(subscription=ai_services_key, region=ai_services_region)\n","    speech_config.speech_recognition_language = \"en-US\"\n","\n","    # Set up the audio configuration using the provided file path\n","    audio_config = speechsdk.audio.AudioConfig(filename=wav_file_path)\n","    # Create a conversation transcriber object\n","    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n","\n","    # Flag to indicate when to stop transcribing\n","    transcribing_stop = False\n","\n","    # Callback for when the transcription session starts\n","    def conversation_transcriber_session_started_cb(evt: speechsdk.SessionEventArgs):\n","        print('SessionStarted event')\n","\n","    # Callback to signal to stop continuous recognition\n","    def stop_cb(evt: speechsdk.SessionEventArgs):\n","        nonlocal transcribing_stop\n","        transcribing_stop = True\n","        # Log the session ID\n","        print(f\"Stopping transcription for session id: {evt.session_id}\")\n","\n","        # Check if the event has a result attribute\n","        if hasattr(evt, 'result'):\n","            # If the result reason is cancellation, provide the cancellation details\n","            if evt.result.reason == speechsdk.ResultReason.Canceled:\n","                cancellation_details = speechsdk.CancellationDetails(evt.result)\n","                print(f\"Transcription was stopped due to cancellation: {cancellation_details.reason}\")\n","                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n","                    print(f\"Error details: {cancellation_details.error_details}\")\n","            # If the result reason is EndOfStream, indicate the audio stream has ended\n","            elif evt.result.reason == speechsdk.ResultReason.EndOfStream:\n","                print(\"Transcription stopped because the end of the audio stream was reached.\")\n","            # If the result reason is NoMatch, indicate no speech could be recognized\n","            elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n","                print(\"Transcription stopped because no speech could be recognized.\")\n","            # For any other reason, log the result reason\n","            else:\n","                print(f\"Transcription stopped for an unknown reason: {evt.result.reason}\")\n","        else:\n","            # If there is no result attribute, log that the reason is unknown\n","            print(\"Transcription stopped, but no additional information is available.\")\n","\n","    # Callback for when the transcription is canceled\n","    def conversation_transcriber_recognition_canceled_cb(evt: speechsdk.SessionEventArgs):\n","        print(\"Canceled event\")\n","        # Access the cancellation details from the event\n","        cancellation_details = speechsdk.CancellationDetails(evt.result)\n","        # Print the reason for the cancellation\n","        print(f\"Canceled event: {cancellation_details.reason}\")\n","\n","        # If there was an error, print the error details\n","        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n","            print(f\"Error details: {cancellation_details.error_details}\")\n","\n","\n","    # Callback for when the transcription session stops\n","    def conversation_transcriber_session_stopped_cb(evt: speechsdk.SessionEventArgs):\n","        # Print the session stopped event with the session id for reference\n","        print(f\"SessionStopped event for session id: {evt.session_id}\")\n","\n","        # If the event has a result attribute, we can check if there are any additional details\n","        if hasattr(evt, 'result') and evt.result:\n","            # Check if the result has a reason attribute and print it\n","            if hasattr(evt.result, 'reason'):\n","                print(f\"Reason for stop: {evt.result.reason}\")\n","\n","            # If the result is a cancellation, print the cancellation details\n","            if evt.result.reason == speechsdk.ResultReason.Canceled:\n","                cancellation_details = speechsdk.CancellationDetails(evt.result)\n","                print(f\"Cancellation reason: {cancellation_details.reason}\")\n","                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n","                    print(f\"Error details: {cancellation_details.error_details}\")\n","\n","\n","    # Handler for the final result of the transcription\n","    def handle_final_result(evt):\n","        nonlocal all_results\n","        # Log the event type\n","        print(f\"Event type: {type(evt)}\")\n","        # Log the event's result reason\n","        print(f\"Result reason: {evt.result.reason}\")\n","\n","        # Check if the event's result is speech recognition with a recognized phrase\n","        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n","            # Parse the JSON result from the transcription\n","            r = json.loads(evt.result.json)\n","            # Log the entire JSON result\n","            print(f\"JSON result: {r}\")\n","            # Append the relevant data to the results list\n","            all_results.append([conversation_id,\n","                                r[\"Id\"],\n","                                r[\"DisplayText\"],\n","                                r[\"Offset\"],\n","                                r[\"Duration\"],\n","                                r[\"Channel\"],\n","                                r[\"Type\"],\n","                                r[\"SpeakerId\"]\n","                                ])\n","        # If the result reason is not recognized speech, log that no recognized speech was found\n","        else:\n","            print(\"No recognized speech was found.\")\n","\n","\n","    # Connect the callbacks to the events fired by the conversation transcriber\n","    conversation_transcriber.transcribed.connect(handle_final_result)\n","    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n","    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n","    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n","    conversation_transcriber.session_stopped.connect(stop_cb)\n","    conversation_transcriber.canceled.connect(stop_cb)\n","\n","    # Start the asynchronous transcription\n","    conversation_transcriber.start_transcribing_async()\n","\n","    # Wait for the transcription to complete\n","    while not transcribing_stop:\n","        time.sleep(.5)\n","\n","    # Stop the asynchronous transcription\n","    conversation_transcriber.stop_transcribing_async()\n","    # Return the list of transcribed results\n","    return(all_results)\n"]},{"cell_type":"code","execution_count":null,"id":"c287e232-e8aa-4726-93d5-4d10f45b9b02","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # spark.sql('drop table if exists ckm_conv_messages')\n","\n","# from pyspark.sql import SparkSession\n","\n","# # Create a Spark session\n","# spark = SparkSession.builder.getOrCreate()\n","\n","# # Get the schema of the existing table\n","# schema = spark.table(\"ckm_conv_messages\").schema\n","\n","# # Create an empty DataFrame with the same schema\n","# empty_df = spark.createDataFrame([], schema)\n","\n","# # Overwrite the existing table with the empty DataFrame\n","# empty_df.write.mode('overwrite').saveAsTable(\"ckm_conv_messages\")\n"]},{"cell_type":"code","execution_count":null,"id":"9551e8b5-47af-40e3-bb33-b50d0f14b491","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["spark.sql('drop table if exists ckm_conv_messages')"]},{"cell_type":"code","execution_count":null,"id":"d7666408-e048-4cab-beed-dc51ddd1ba86","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["\"\"\"\n","This script transcribes audio files using the AI services key and region. It iterates over each row in a dataframe, \n","constructs the full path of the audio file, and attempts to transcribe the audio. If the transcription is successful \n","and not empty, it creates a new dataframe with the transcriptions and writes it to a delta table 'ckm_conv_messages'. \n","The 'ckm_conv_messages' table stores the conversation messages with columns such as conversation_id, Id, DisplayText, \n","Offset, Duration, Channel, Type, and SpeakerId. The processed audio files are then moved to a 'audio_processed' folder. \n","If an error occurs during the process, it prints the error message and the file that could not be loaded.\n","\"\"\"\n","\n","from pyspark.sql import functions as f\n","\n","for row in df.rdd.collect():\n","    # Strip leading and trailing whitespace from the file name\n","    file_name = row.FileName.strip()\n","    wav_file_path = '/lakehouse/default/Files/data/audio_input/' + file_name # full path is required for speechSDK\n","    # print(wav_file_path)\n","    try:\n","        # print(f\"transcribing file: {wav_file_path}\")\n","        r = transcribe_from_file(ai_services_key,ai_services_region,wav_file_path,row.ConversationId)\n","        # print(f\"r= {r}\")\n","        if len(r) != 0:\n","            df_columns = [\"conversation_id\",\"Id\",\"DisplayText\",\"Offset\",\"Duration\",\"Channel\",\"Type\",\"SpeakerId\"]\n","            df_conv = spark.createDataFrame(data=r, schema = df_columns)\n","            df_conv = df_conv.coalesce(1).withColumn(\"row_id\", f.monotonically_increasing_id())\n","\n","            df_conv.write.format('delta').mode('append').option(\"overwriteSchema\", \"true\").saveAsTable('ckm_conv_messages')\n","            # Move the processed file to the 'audio_processed' folder\n","            mssparkutils.fs.mv(('Files/data/audio_input/' + file_name), ('Files/data/audio_processed/' + file_name), False,True)\n","            # break\n","    except Exception as e:\n","        print(\"could not load:\", wav_file_path)\n","        print(\"An error occurred:\", e)  # Print the exception\n","        # Move the processed file to the 'audio_failed' folder\n","        mssparkutils.fs.mv(('Files/data/audio_input/' + file_name), ('Files/data/audio_failed/' + file_name), False,True)\n"]},{"cell_type":"code","execution_count":null,"id":"3cdf3ba5-0089-4ad5-b6c7-0a38cdcb50ff","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import shutil\n","\n","# Directory paths\n","input_dir = '/lakehouse/default/Files/data/audio_input/'\n","processed_dir = '/lakehouse/default/Files/data/audio_processed/'\n","\n","# Get a list of all .csv files in the input directory\n","csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n","\n","# Move each .csv file to the processed directory\n","for file_name in csv_files:\n","    shutil.move(os.path.join(input_dir, file_name), os.path.join(processed_dir, file_name))\n"]}],"metadata":{"dependencies":{"environment":{},"lakehouse":{"default_lakehouse":"e6ad9dad-e3da-4da5-bca6-6572c466b69a","default_lakehouse_name":"ckm_lakehouse","default_lakehouse_workspace_id":"0d98d480-171b-4b4d-a8e7-80fbd031d1a6"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
