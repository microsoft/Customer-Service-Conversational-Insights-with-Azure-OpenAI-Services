{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042c33d-d831-4da4-890d-da2fb8e3d3c8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech==1.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5e38d-0232-4336-9d64-0c0a3687efed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# key_vault_name value is set at the time of deployment\n",
    "\n",
    "key_vault_name = 'kv_to-be-replaced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34455249-5154-4cf8-b102-abcef9ca9d89",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n",
    "\n",
    "def get_secrets_from_kv(kv_name, secret_name):\n",
    "\n",
    "    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n",
    "    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n",
    "    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))\n",
    "\n",
    "openai_api_type = \"azure\"\n",
    "openai_api_version  = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-VERSION\")\n",
    "openai_api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n",
    "openai_api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d20f3e-8425-4ed7-add7-2056418afdb9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Set AI services variables\n",
    "ai_services_endpoint = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-ENDPOINT\") \n",
    "ai_services_key = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-KEY\") \n",
    "ai_services_region = get_secrets_from_kv(key_vault_name,\"COG-SERVICES-REGION\")\n",
    "# wav_file_path = '/lakehouse/default/Files/data/audio_input/Travel_20240417132839.wav'\n",
    "language1 = 'en-US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6c88e-45f7-4e5b-829c-8d3bdd95de25",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # This cell creates new folders within the specified base path in the lakehouse. \n",
    "# The purpose is to create corresponding folders so files can be moved as they are processed.\n",
    "import os \n",
    "\n",
    "# Define the base path\n",
    "base_path = '/lakehouse/default/Files/data'\n",
    "\n",
    "# List of folders to be created\n",
    "folders = ['audio_failed', 'audio_processed']\n",
    "\n",
    "# Create each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        print(f'Folder created at: {folder_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to create the folder {folder_path}. Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cacd3-e46b-49dd-a13a-1f85477dda39",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Drop the metadata table if it already exists\n",
    "spark.sql('drop table if exists ckm_conv_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3c4a2-e888-4bd8-add1-fb08f42af75c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# Read all the CSV files in the directory\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/audio_input/*.csv\")\n",
    "\n",
    "# Convert StartTime and EndTime to timestamp format\n",
    "df = df.withColumn(\"StartTime\", F.to_timestamp(\"StartTime\", \"MM/dd/yyyy h:mm:ss a\"))\n",
    "df = df.withColumn(\"EndTime\", F.to_timestamp(\"EndTime\", \"MM/dd/yyyy h:mm:ss a\"))\n",
    "\n",
    "# Calculate the duration in milliseconds and add it as a new column\n",
    "df = df.withColumn(\"Duration\", (F.col(\"EndTime\").cast(\"long\") - F.col(\"StartTime\").cast(\"long\")) / 60)\n",
    "\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('ckm_conv_metadata')\n",
    "\n",
    "# # Display the first 2 rows\n",
    "# display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f17c4-fc8d-4b0d-9ea1-aa72bd640963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This code block is designed to transcribe speech from an audio file using Azure's Cognitive Services Speech SDK.\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization?tabs=windows&pivots=programming-language-python\n",
    "\n",
    "# It supports diarization, which distinguishes between different speakers in the audio.\n",
    "# The transcribed results are returned as a list of all recognized utterances with associated metadata.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Function to transcribe speech from an audio file\n",
    "def transcribe_from_file(ai_services_key, ai_services_region, wav_file_path, conversation_id):\n",
    "    # List to store the results of the transcription\n",
    "    all_results = list()\n",
    "\n",
    "    # Configure the speech service\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=ai_services_key, region=ai_services_region)\n",
    "    speech_config.speech_recognition_language = \"en-US\"\n",
    "\n",
    "    # Set up the audio configuration using the provided file path\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=wav_file_path)\n",
    "    # Create a conversation transcriber object\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Flag to indicate when to stop transcribing\n",
    "    transcribing_stop = False\n",
    "\n",
    "    # Callback for when the transcription session starts\n",
    "    def conversation_transcriber_session_started_cb(evt: speechsdk.SessionEventArgs):\n",
    "        print('SessionStarted event')\n",
    "\n",
    "    # Callback to signal to stop continuous recognition\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        nonlocal transcribing_stop\n",
    "        transcribing_stop = True\n",
    "        # Log the session ID\n",
    "        print(f\"Stopping transcription for session id: {evt.session_id}\")\n",
    "\n",
    "        # Check if the event has a result attribute\n",
    "        if hasattr(evt, 'result'):\n",
    "            # If the result reason is cancellation, provide the cancellation details\n",
    "            if evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "                cancellation_details = speechsdk.CancellationDetails(evt.result)\n",
    "                print(f\"Transcription was stopped due to cancellation: {cancellation_details.reason}\")\n",
    "                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                    print(f\"Error details: {cancellation_details.error_details}\")\n",
    "            # If the result reason is EndOfStream, indicate the audio stream has ended\n",
    "            elif evt.result.reason == speechsdk.ResultReason.EndOfStream:\n",
    "                print(\"Transcription stopped because the end of the audio stream was reached.\")\n",
    "            # If the result reason is NoMatch, indicate no speech could be recognized\n",
    "            elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "                print(\"Transcription stopped because no speech could be recognized.\")\n",
    "            # For any other reason, log the result reason\n",
    "            else:\n",
    "                print(f\"Transcription stopped for an unknown reason: {evt.result.reason}\")\n",
    "        else:\n",
    "            # If there is no result attribute, log that the reason is unknown\n",
    "            print(\"Transcription stopped, but no additional information is available.\")\n",
    "\n",
    "    # Callback for when the transcription is canceled\n",
    "    def conversation_transcriber_recognition_canceled_cb(evt: speechsdk.SessionEventArgs):\n",
    "        print(\"Canceled event\")\n",
    "        # Access the cancellation details from the event\n",
    "        cancellation_details = speechsdk.CancellationDetails(evt.result)\n",
    "        # Print the reason for the cancellation\n",
    "        print(f\"Canceled event: {cancellation_details.reason}\")\n",
    "\n",
    "        # If there was an error, print the error details\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "\n",
    "    # Callback for when the transcription session stops\n",
    "    def conversation_transcriber_session_stopped_cb(evt: speechsdk.SessionEventArgs):\n",
    "        # Print the session stopped event with the session id for reference\n",
    "        print(f\"SessionStopped event for session id: {evt.session_id}\")\n",
    "\n",
    "        # If the event has a result attribute, we can check if there are any additional details\n",
    "        if hasattr(evt, 'result') and evt.result:\n",
    "            # Check if the result has a reason attribute and print it\n",
    "            if hasattr(evt.result, 'reason'):\n",
    "                print(f\"Reason for stop: {evt.result.reason}\")\n",
    "\n",
    "            # If the result is a cancellation, print the cancellation details\n",
    "            if evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "                cancellation_details = speechsdk.CancellationDetails(evt.result)\n",
    "                print(f\"Cancellation reason: {cancellation_details.reason}\")\n",
    "                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                    print(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "\n",
    "    # Handler for the final result of the transcription\n",
    "    def handle_final_result(evt):\n",
    "        nonlocal all_results\n",
    "        # Log the event type\n",
    "        print(f\"Event type: {type(evt)}\")\n",
    "        # Log the event's result reason\n",
    "        print(f\"Result reason: {evt.result.reason}\")\n",
    "\n",
    "        # Check if the event's result is speech recognition with a recognized phrase\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            # Parse the JSON result from the transcription\n",
    "            r = json.loads(evt.result.json)\n",
    "            # Log the entire JSON result\n",
    "            print(f\"JSON result: {r}\")\n",
    "            # Append the relevant data to the results list\n",
    "            all_results.append([conversation_id,\n",
    "                                r[\"Id\"],\n",
    "                                r[\"DisplayText\"],\n",
    "                                r[\"Offset\"],\n",
    "                                r[\"Duration\"],\n",
    "                                r[\"Channel\"],\n",
    "                                r[\"Type\"],\n",
    "                                r[\"SpeakerId\"]\n",
    "                                ])\n",
    "        # If the result reason is not recognized speech, log that no recognized speech was found\n",
    "        else:\n",
    "            print(\"No recognized speech was found.\")\n",
    "\n",
    "\n",
    "    # Connect the callbacks to the events fired by the conversation transcriber\n",
    "    conversation_transcriber.transcribed.connect(handle_final_result)\n",
    "    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n",
    "    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n",
    "    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n",
    "    conversation_transcriber.session_stopped.connect(stop_cb)\n",
    "    conversation_transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start the asynchronous transcription\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Wait for the transcription to complete\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    # Stop the asynchronous transcription\n",
    "    conversation_transcriber.stop_transcribing_async()\n",
    "    # Return the list of transcribed results\n",
    "    return(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287e232-e8aa-4726-93d5-4d10f45b9b02",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # spark.sql('drop table if exists ckm_conv_messages')\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Get the schema of the existing table\n",
    "# schema = spark.table(\"ckm_conv_messages\").schema\n",
    "\n",
    "# # Create an empty DataFrame with the same schema\n",
    "# empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# # Overwrite the existing table with the empty DataFrame\n",
    "# empty_df.write.mode('overwrite').saveAsTable(\"ckm_conv_messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551e8b5-47af-40e3-bb33-b50d0f14b491",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('drop table if exists ckm_conv_messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7666408-e048-4cab-beed-dc51ddd1ba86",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script transcribes audio files using the AI services key and region. It iterates over each row in a dataframe, \n",
    "constructs the full path of the audio file, and attempts to transcribe the audio. If the transcription is successful \n",
    "and not empty, it creates a new dataframe with the transcriptions and writes it to a delta table 'ckm_conv_messages'. \n",
    "The 'ckm_conv_messages' table stores the conversation messages with columns such as conversation_id, Id, DisplayText, \n",
    "Offset, Duration, Channel, Type, and SpeakerId. The processed audio files are then moved to a 'audio_processed' folder. \n",
    "If an error occurs during the process, it prints the error message and the file that could not be loaded.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "for row in df.rdd.collect():\n",
    "    # Strip leading and trailing whitespace from the file name\n",
    "    file_name = row.FileName.strip()\n",
    "    wav_file_path = '/lakehouse/default/Files/data/audio_input/' + file_name # full path is required for speechSDK\n",
    "    # print(wav_file_path)\n",
    "    try:\n",
    "        # print(f\"transcribing file: {wav_file_path}\")\n",
    "        r = transcribe_from_file(ai_services_key,ai_services_region,wav_file_path,row.ConversationId)\n",
    "        # print(f\"r= {r}\")\n",
    "        if len(r) != 0:\n",
    "            df_columns = [\"conversation_id\",\"Id\",\"DisplayText\",\"Offset\",\"Duration\",\"Channel\",\"Type\",\"SpeakerId\"]\n",
    "            df_conv = spark.createDataFrame(data=r, schema = df_columns)\n",
    "            df_conv = df_conv.coalesce(1).withColumn(\"row_id\", f.monotonically_increasing_id())\n",
    "\n",
    "            df_conv.write.format('delta').mode('append').saveAsTable('ckm_conv_messages')\n",
    "            # Move the processed file to the 'audio_processed' folder\n",
    "            mssparkutils.fs.mv(('Files/data/audio_input/' + file_name), ('Files/data/audio_processed/' + file_name), False,True)\n",
    "            # break\n",
    "    except Exception as e:\n",
    "        print(\"could not load:\", wav_file_path)\n",
    "        print(\"An error occurred:\", e)  # Print the exception\n",
    "        # Move the processed file to the 'audio_failed' folder\n",
    "        mssparkutils.fs.mv(('Files/data/audio_input/' + file_name), ('Files/data/audio_failed/' + file_name), False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf3ba5-0089-4ad5-b6c7-0a38cdcb50ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Directory paths\n",
    "input_dir = '/lakehouse/default/Files/data/audio_input/'\n",
    "processed_dir = '/lakehouse/default/Files/data/audio_processed/'\n",
    "\n",
    "# Get a list of all .csv files in the input directory\n",
    "csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Move each .csv file to the processed directory\n",
    "for file_name in csv_files:\n",
    "    shutil.move(os.path.join(input_dir, file_name), os.path.join(processed_dir, file_name))\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "e6ad9dad-e3da-4da5-bca6-6572c466b69a",
    "default_lakehouse_name": "ckm_lakehouse",
    "default_lakehouse_workspace_id": "0d98d480-171b-4b4d-a8e7-80fbd031d1a6"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
